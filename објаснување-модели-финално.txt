
R^2 -> R squared 
R^2 претставува коефициент на детерминација или квадратна грешка која помага во тоа да ги увидиме
перформансите на еден модел, вообичаено се користи кај регресиски модели(модел на линеарна регресија)
Покажува колку добро моделот ја објаснува зависната променливс Y (која ја предвидуваме) врз основа на
независните променливи X. Доколку вредноста нејзина е 0, ова значи моделот e добар, односно
независните променливи не даваат никакви информации за променливата што ја предивудаме, но доколку
нејзината вредност е 1 , значи моделот е добар и дека целосно ја објаснува варијансата во зависната променлива.
-------------------------------------------------------------------------------------------------------------------
Класификација 
Линеарната регрсија добро функционира доколку променливата што ја предвиудаваме е од нумерички тип, но кога
променливите кои ги предвидуваме се од категориски тип, проблемот не е повеќе регресивен проблем , туку проблем на
класификација. Целта наша е да се обидеме да ја класифицираме променливата Y позната како класа/кластер врз основа
на множество од променливи на предивудање X. Класификацијата може да биде бинарна класификација пример доколку
предвидуваме дали личноста е машко или женско, дали ќе добие кредит или не итн. мултилабел класификација каде имаме 
повеќе од 2 вредности на излез.
-----------------------------------------------------------------------------------------------------------------
Важни мерки за перформанси на модели (Accuracy, Precision, F1-Score, Recall)
1)Precision - пример предвидуваме јаболка и круши -> колку од сите јаболка што сме ги предвиделе како јаболка 
се навистина јаболка ; 
Ex. ако предвидиме 10 јаболка, а само 7 се навистина јаболка тогаш прецизноста е :
  	[ TP / TP + FP] <=> [ 7 / 7 + 3 ] => 0.7 ( 70%)
2) Recall(отповик) - пример предвидуваме јаболка и круши -> колку од вкупното овошје успеал моделот да лабелира
примероци како јаболка
Ex. ако имаме 7 јаболка, а моделот открил правилно 5 од нив тогаш :
 	[ TP / TP + FN ] <=> [5 / 5 + 2 ] => 0.71 ( 71%)
3) Accuracy(точност) - покажува колкав процент од сите предвидување ( позитивни и негативни) на моделот се точни.
	[TP + TN / all samples]
4) F1-score - е баланс помеѓу precision и recall(од него можеме да видиме дали еден dataset е балансиран/не)
-----------------------------------------------------------------------------------------------------------------
	Што е стандардизација, што е нормализација?

-Стандардизација е процес на скалирање на податоците така што тие ќе имаат средна вредност(просек) = 0 и
стандардна девијација = 1. Стандардизација вршиме за да ги направиме карактеристиките (features) споредливи
доколку имаат различни параметри или единици. Се користи особено кај алгоритми како КNN, логистичка регресија итн.
-Нормализација е процес на скалирање на податоците така што сите вредности ќе бидат во опсег од 0 до 1. Ова го правиме
со цел да се осигураме дека карактеристиките се во ист опсег.
-Ова го правиме бидејќи многу алгоритми во машинското учење учат подобро и работат подобро и побрзо кога карактеристиките 
се слични и исто така го подобруваме перформансот и точноста на моделите.
---------------------------------------------------------------------------------------------------------------------
	Зошто balanced DataSet Accuracy не е корисна?(BDA) 

Не е корисна од неколку причини
-Не се применува доколку нашиот податочен сет е веќе балансиран(содржи примероци кои припаѓаат подеднакво на сите класи)
Во овој случај обичната Accuracy ни е доволна.
-Го зима во предвид просекот на точноста за секоја класа, меѓутоа ова не ја вклучува важноста на разни класи во конкретни проблеми
------------------------------------------------------------------------------------------------------------------------

1. Модел на линеарна регресија
Моделот на линеарна регресија вообичаено се користи за нумерички тип на променливи(континуирани).
Овој модел можеме да го примениме доколку извршиме анализа на распределбата на податоците и видиме
дека постои корелација, односно некаква линеарна зависност помеѓу променливата која ја предвидуваме Y
и независните променливи X. Целта на овој модел е да се предвиди зависноста помеѓу зависната променлива врз 
основа на познатите независни променливи, односно нивното влијание.
Овој модел користи равенка на права линија за да ги предвиди вредностите
 Y = β0 + β1*x +  ε
Пример можеме да го искористиме доколку сакаме да видиме како плоштината на една куќа 
ќе влијае врз нејзината цена, во овој случај независни променливи X ќе бидат плоштините,
од друга страна зависна променлива која ќе ја предвидуваме ќе биде цената.

2. КNN модел за класификација
Овој е еден од најпопуларните модели за класификација, каде се користи за 2та типа на променливи,
но вообичаено се користи доколку зависната променлива Y која ја превидуваме е од категориски тип.
Предвидува излез за нов примерок врз основа на неговите најблиски соседи во податочниот сет.
Тој функционира на начин што го мери растојанието од новата точка до сите точки во тренинг податоците.
(бараме менхетенско растојание) Ги наоѓа најблиските к-соседи до новата точка , бидејќи станува збор
за класификација ја избира најчестата класа од тие к-точки, но доколку е регресија, го зима просекот
за нивните вредности.

3.Модел на логистичка регресија
Логистичката регресија е статистички модел кој се користи за класификаицја, односно кога
зависната променливплати кредит врз база на неговата кредитна историја итн. 
Таа користи логистичка функција која претвора линеарна комбинација на независни променливи во 
вредност помеѓу 0 и 1. Ако резултатот е поголем од 0.5 вообичаено се класифицира како 1 (позитивен случај)
, ако е помал од 0.5 се клаисифицира како 0 (негативен случај)
Сепак, недостаток на овој модел е тоа што е чувствителен на аутлаер.

4.Модел на полиномијална регресија
Полиномијалната регресија е еден вид проширување на линеарната регресија, со таа разлика што
се јавува доколку постои нелинеарна зависност помеѓу влезот и излезот. Наместо тоа што кај
линеарната регресија се користеше линеарна комбинација на независните променливи , да ги најдеме
B0, B1.. тука имаме и хиперпараметар(метаподаток) кој ни дава можност да одбереме параметар(полином)
кој што ќе ни го оформи моделот. Пример хиперпараметар кај КНН претставува бројот на соседи,
тука ни е x на некој степен (X^n)
Пример во еден координатен систем имаме 6 точки и ние можеме да креираме полиномна функција
која ќе мине низ сите 6 точки, односно да креираме модел кој ќе ги превиди со 100 % точност,но моделот 
нема да може добро да предвидува нови непознати примероци , со ова грешките кај train множеството
ќе се намалата, но ќе се зголемат кај test множеството, односно ова може да доведе до нов проблем
познат како overfitting (кога доведувањето на подобрување
на перформансите на моделот на тренинг множеството води кон влошување на перформансите на моделот 
на тест множеството.

5. Decisiton Tree модел 
Моделот на decision tree се состои од 3 дела :
1) коренот на дрвото(root) - ги содржи сите податоци
2) внатрешни јазли(internal nodes) - се создаваат врз основа на прашања или услови засновани на некој feature на податоците
3) листовите - конечна одлука или предвидување
како функционира моделот?
1) поделба на податоците (splitting) 
дрвото одлучува како ќе ги подели податоците врз основа на некоја нивна карактеристика(feature) , тука се користат разни метрики како gini index(за колку е чист одреден јазол), ентропија, mean squered error(за регресија, мери разлика меѓу предвидените вредности и вистинските вредности )
2) рекурзија - се повторува процесот се додека не се исполни услов за стоп(пр. достигната е максимална длабочина на дрво, нема веќе податоци за делење, класите се чисти)
3) одлука - секој лист претставува крајна одлука, за класификација најчесто е некоја класа, за регреија е некоја просечна вредност
- се користи и за категориски и за континуирани променливи
- не бара претходна нормализација на податоците
Џини индекс е мерка која се користи кај decision trees за да одреди колкава е хомогеноста(чистотата) во еден јазол , помага да се одбере кој feauture е  најдобар за да се избере при поделба на податоците
- колку е помал џини индексот толку е почист јазолот(node); 
чист јазол - сите податоци припаѓаат на една класа (џини = 0)
нечист јазол - податоците се мешавина од повеќе класи

____________
Bootstraping е статистички метод кој се користи за проценка на распределбата на примероците и за добивање на доверливи интервали или статистички проценки, користејќи повторно примероци од оригиналните податоци. Тоа вклучува случајно земање примероци со замена од оригиналниот податочен сет (т.е. истиот податок може да се појави повеќе пати во новиот примерок). Овој метод е корисен кога не се има претходна претпоставка за обликот на распределбата или кога работиме со мали примероци.
-----------
Што е Полиномијална Регресија?
Полиномијална регресија е метод за регресија кој се користи за моделирање на зависноста помеѓу зависната и независната променлива преку полиномски функции. Тоа е проширување на линeарната регресија, каде што моделот не е ограничен на права линија, туку може да биде кривулја која најдобро ја прилагодува зависноста.
•	Преголема флексибилност (overfitting): Кога се користат високи степени на полиномијал, моделот може да се прилагоди премногу на податоците и да не генерализира добро на нови податоци.
•	Тешкотија при интерпретација: Како што се зголемува степенот на полиномијалната регресија, така и интерпретацијата на моделот станува потешка.
-----------------------------------------
Overfitting (прекумерно прилагодување) се јавува кога моделот премногу се прилагодува на специфичните податоци за тренинг, што доведува до ниска грешка на тренинг сетот, но лоша способност за предвидување на нови, невидени податоци. Ова може да се случи кога моделот е премногу сложен и вклучува многу карактеристики или високи степени на полиномијални функции, што резултира со "приспособување" на секој мал варијабилитет во податоците, вклучувајќи и случајни флуктуации кои не претставуваат вистински образец. Може да се избегне со користење на крос-валидација, намалување на сложеноста на моделите, како и со зголемување на тренинг множеството со што моделот ќе може да се тренира на повеќе различни случаи.

oТехниките за регуларизација, како L1 (Lasso) и L2 (Ridge) регуларизација, додаваат казни за големите вредности на коефициентите и помагаат да се задржи моделот поедноставен.
-------------------------------------------------------------------------------------------------
Cross Validation(крос-валидација)
Оваа техника се користи за избегнување на overfitting кај повеќе модели(пр. линеарна регресија,
логистичка регресија, КНН итн. Но, исто така и за проценка на перфомрансите на моделите.
Функционира на начин што првен се дели дата сетот на к подмножества(folds).
Моделот се тренира к-1 пати , секогаш се користи различен сет за тестирање, се прават
к итерации каде во секоја итерација секој fold барем еднаш ја има улогата на test сет.
На крај на секоја итерација се пресметува средна вредност од секој fold.
Секој податок точно еднаш се наоѓа во секој тест сет, и е тестиран к-1 пати
---------------------------------------------------------------------------------------------------------
K-Fold Cross Validation (K-Fold CV) е техника за проценка на перформансите на моделите во машинското учење, која се користи за да се добијат поопшти проценки за точноста на моделот и да се намали ризикот од прекумерно прилагодување (overfitting). Оваа техника ја дели целокупната обучувачка серија на k подмножества (folds) и секое од нив ја добива улогата на тест сет на различен начин.
oНа почетокот, податоците се разделуваат на k подмножества (folds), каде што секое подмножество е еднакво големо. На пример, ако имате 1000 примероци на податоци и изберете k=5, тогаш ќе имате 5 подмножества, секое со 200 примероци.

----------------
oBias претставува разликата помеѓу просечната прогноза на моделот и вистинската вредност која сакаме да ја предвидиме.
  Висок bias значи дека моделот има погрешни претпоставки за природата на податоците и може да направи едноставни погрешни проценки. Моделот не успева да ја улови сложеноста на податоците

oVariance се однесува на променливоста на моделот кога се користат различни тренинг сетови.
     Модел со висок variance има способност да се приспособува премногу на малите флуктуации во тренинг податоците (шум), што значи дека ќе направи многу добри прогнози на тренинг сетот, но лоши на нови податоци.
     Високиот variance значи дека моделот е премногу сложен и многу чувствителен на тренинг податоците.
-------------------
Ridge regression - е метод за регресија кој добро работи ако имаме висока корелација меѓу променливите.
Воедно ги прави решенијата постабилни. Се користи ако сакаме да ги минимизараме грешките на предвидување, ако повеќето независни променливи имаат влијание врз зависната променлива, голем број значајни карактеристики.

Lasso regression - ако мислиме дека само дел од карактериситките се значајни. Може да се користи за поедноставување на моделот. Се користи доколку сакаме автоматски избор на карактеристиките и лесна интерпретација.

Регуларизациониот параметар \( \lambda \) ја контролира тежината на казнување за големи вредности на параметрите \( b \). 

- Ако \( \lambda \) е близу до нула, ја враќаме стандардната MSE (средна квадратична грешка), што значи дека Ridge и LASSO регресијата се сведуваат на обична регресија.
- Ако \( \lambda \) е доволно голем, термините за MSE ќе станат незначајни, а регуларизацијата ќе ги натера параметрите \( b \) да бидат близу нула.
- За да се избегне произволен избор на \( \lambda \), треба да го избереме преку крос-валидација.
________________________________
Одлучувачки дрвја (Decision Trees)

Одлучувачките дрвја се едни од најпопуларните и најразбирливите модели за класификација и регресија во машинското учење. Тие ги делат податоците на различни сегменти преку серија на прашања (т.е. тестови) на различни атрибути, што создава структура која изгледа како дрво. За секој внатрешен јазол во дрвото, има прашање кое дели податоците во две или повеќе групи, додека на крајот на дрвото се наоѓаат листи кои го претставуваат финалниот резултат (класа или вредност).

Структура на одлучувачко дрво
1.	Корен (Root Node): Почетниот јазол во дрвото кој претставува целиот сет на податоци. Овој јазол го поставува првиот прашање за поделба на податоците.
2.	Внатрешни јазли (Internal Nodes): Секој внатрешен јазол претставува тест на еден атрибут (карактеристика). Во овој јазол, податоците се делат на две или повеќе подгрупи врз основа на резултатите од тестот.
3.	Листови (Leaf Nodes): На крајот на дрвото се листовите, кои претставуваат конечни класификација или предвидување на вредноста (за регресија).
4.	Гранки (Branches): Гранките поврзуваат јазли и покажуваат како податоците се делат според резултатот од тестовите.
Процесот на изградба на одлучувачко дрво вклучува следниве чекори:


1.	Поделба на податоците:
 На почетокот, сите податоци се во еден јазол (корен). За да се изгради дрвото, треба да се изберат атрибути и прашања кои ќе ја поделат податочната сетка на групи.
2.	Избор на најдобар атрибут: За секој внатрешен јазол, треба да се избере атрибут врз основа на кој ќе се направи поделба на податоците. Овој избор најчесто се прави со примена на критериуми за избор како што се: 
o	Gini Index (за класификација) - meri cistota vo edeen jazol
o	Entropy или Information Gain (за класификација)
o	Mean Squared Error (MSE) (за регресија)
3.	Рекурзивна поделба: Постапката за избор на атрибути и поделба се повторува рекурзивно за секој нов јазол. Процесот завршува кога: 
o	Податоците во јазолот се од истата класа (за класификација).
o	Јазолот има премалку податоци за да продолжи поделбата.
o	Нема други атрибути за тестирање.

Предности:
•	Интерпретабилност: Одлучувачките дрвја се лесни за разбирање и визуелизација, што ги прави одлични за задачи каде објаснувањето на моделот е важно.
•	Не бараат нормализација: Атрибутите не треба да бидат нормализирани или стандартизирани, бидејќи одлучувачките дрвја работат со релативните вредности на атрибутите.
Недостатоци:
•	Прекумерно приспособување (Overfitting): Одлучувачките дрвја често можат да се прекумерно приспособат на тренинг податоците, особено ако се дозволат многу длабоки дрвја.
•	Несигурност: Одлучувачките дрвја можат да бидат нестабилни, бидејќи мали промени во податоците може да доведат до значајни промени во структурата на дрвото.
Како да се реши прекумерното приспособување?
•	Прекратување (Pruning): Прекратувањето на дрвото се врши за да се избегне прекумерно приспособување. Ова може да се направи преку елиминирање на некои гранки кои не придонесуваат значително за точноста на моделот.
•	Минимален број на примероци за поделба: Поставување минимален број на примероци кои треба да бидат присутни во еден јазол за да може да се изврши поделба.
•	Максимална длабочина на дрвото: Ограничување на длабочината на дрвото може да помогне да се спречи создавање на многу комплексни и нестабилни модели.
Со оваа структура и процес, одлучувачките дрвја се многу моќен алат во машинското учење и се користат за различни задачи како што се класификација, регресија и сегментација на податоци.

